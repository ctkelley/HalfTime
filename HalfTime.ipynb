{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38b82d8",
   "metadata": {},
   "source": [
    "This notebook is a record of what I am doing with half precision. The first task is to show that on Apple M* hardware that half is faster than single is faster than double. You won't see exactly the factor of 2 you'd expect because the execution times are so low.\n",
    "\n",
    "I'm using the Julia ```sum``` command for these tests. Any of your enhanced summaion funcitons should show  similar effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909a1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the init script\n",
    "include(\"Codes/HalfTimeInit.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe488675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Float16 array and copy it to Float32 and Float64\n",
    "N=4096\n",
    "x=rand(Float16,N); x32=Float32.(x); x64=Float64.(x);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1627f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  139.856 ns (0 allocations: 0 bytes)\n",
      "  199.722 ns (0 allocations: 0 bytes)\n",
      "  369.362 ns (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Time the Julia sum command\n",
    "@btime sum($x);\n",
    "@btime sum($x32);\n",
    "@btime sum($x64);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a241c",
   "metadata": {},
   "source": [
    "Next I want to look at the errors. I will take ```sum(x64)``` as truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7714164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float16 error = 1.02710e-04; Float32 error = 0.00000e+00\n"
     ]
    }
   ],
   "source": [
    "s=sum(x); s32=sum(x32); s64=sum(x64);\n",
    "e16=abs(s - s64)/s64\n",
    "e32=abs(s32 - s64)/s64\n",
    "println(\"Float16 error = $e16; Float32 error = $e32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a65fd",
   "metadata": {},
   "source": [
    "For summing a __short__ sequence of half precision numbers, promoting to single is enough to get the error exactly correct.\n",
    "\n",
    "The next thing is to look at my hack job to avoid overflow and reduce any memory burden in the summation. The plan\n",
    "is to sum the vector in blocks. So N is the size of the vector, NB is the number of blocks, and M is the block size. I'm assuming that N = NB * M and things like ```@simd``` will work better if M is a multiple of 512.\n",
    "\n",
    "Here's an example to show how you can cheat on overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e29a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float16 sum = Inf; Truth = 1.31447e+05\n"
     ]
    }
   ],
   "source": [
    "N = 512 * 512; x=rand(Float16,N); x64=Float64.(x);\n",
    "s=sum(x); x64=sum(x64);\n",
    "println(\"Float16 sum = $s; Truth = $x64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3522142",
   "metadata": {},
   "source": [
    "The sum really does overflow, but accumulating the sum is double gets the right result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
